{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segédkönyvtár arcok alapján való osztályozáshoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Előkészületek és a háló elkészítése"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szükséges csomagok hivatkozása."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:\\\\users\\\\ifjto\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nézzük meg, hogy tudunk-e videókártyán futtatni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available!\")\n",
    "else:\n",
    "    print(\"CUDA not available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neurális háló reziduális blokkját megvalósító osztály. A neurális háló a ResNet architektúrát használja. Azért esett erre a híres típusra a választásom, mert ez a neurális hálók képességeit nagyban növelte, de bonyolultsága és számításigénye mégsem óriási. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, nFeat, layersPerLevel, kernelSize):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(layersPerLevel):\n",
    "            layer = nn.Sequential( \n",
    "                nn.Conv2d(nFeat, nFeat, kernelSize, padding=kernelSize//2, bias=False),\n",
    "                nn.BatchNorm2d(nFeat),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        self.under_scaling = nn.Conv2d(nFeat, 2*nFeat, kernelSize, padding=kernelSize//2, stride=2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)\n",
    "        for layer in self.layers[1:]:\n",
    "            out = layer(out)\n",
    "        out += x\n",
    "        out = self.under_scaling(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neurális hálót reziduális blokkokból összerakó osztály."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, inCh, nC, nFeat, nLevels, layersPerLevel, kernelSize, dropout):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.first_layer = nn.Conv2d(inCh, nFeat, kernelSize, padding=kernelSize//2, bias=False)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(nLevels):\n",
    "            level = ResidualBlock(nFeat*pow(2, i), layersPerLevel, kernelSize)\n",
    "            self.blocks.append(level)\n",
    "        self.pooling_layer = nn.AdaptiveAvgPool2d(10)\n",
    "        self.dropout_layer = nn.Dropout2d(p=dropout)\n",
    "        self.linear_layer = nn.Linear(100*pow(2, nLevels)*nFeat, nC)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        for level in self.blocks:\n",
    "            x = level(x)\n",
    "        x = self.pooling_layer(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tanulóadatok beolvasása és előfeldolgozása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olvassuk be a képeket, és címkézzük fel. Erre hozzunk létre egy Dataset objektumot. Beolvasáskor normáljuk is a képeket a numerikus konvergencia támogatásáért. Mérjük le az egyes beolvasott adatok relatív gyakoriságának reciprokát, hogy később ezzel súlyozva a hálónk jobban tudjon tanulni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, attribute, accepted_values):\n",
    "        self.face = []\n",
    "        self.label = []\n",
    "        self.frequencies = [0]*len(accepted_values)\n",
    "\n",
    "        pictures = {}\n",
    "        for pic in os.listdir(\"cropped64\"):\n",
    "            image = cv2.imread(\"cropped64/\"+pic, cv2.IMREAD_GRAYSCALE)\n",
    "            norm_image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            pictures[pic] = torch.Tensor( norm_image ).unsqueeze(0)\n",
    "\n",
    "        with open(\"processed.txt\", \"r\") as file:\n",
    "            not_accepted = {}\n",
    "            for z, record in enumerate(file.readlines()):\n",
    "                parts = record[:-1].split(\";\")\n",
    "                attributes = {}\n",
    "                for i in parts:\n",
    "                    attributes[i.split(\":\")[0]] = i.split(\":\")[1]\n",
    "                if attributes[\"pic\"] != \"None\":\n",
    "                    if attributes[\"pic\"] in pictures.keys():\n",
    "                        \n",
    "                        if attributes.get(attribute) is None:\n",
    "                            print(\"Missing attribute \"+attribute)\n",
    "                            continue\n",
    "                        \n",
    "                        # if the record is acceptable save it\n",
    "                        if attributes[attribute] in accepted_values:\n",
    "                            self.face.append(pictures[attributes[\"pic\"]])\n",
    "                            ID = accepted_values.index(attributes[attribute])\n",
    "                            self.label.append(ID)\n",
    "                            self.frequencies[ID] += 1\n",
    "                        \n",
    "                        # if the record isn't accepted but correct, log it\n",
    "                        else:\n",
    "                            if attributes[attribute] in not_accepted.keys():\n",
    "                                not_accepted[attributes[attribute]] += 1\n",
    "                            else:\n",
    "                                not_accepted[attributes[attribute]] = 1\n",
    "            \n",
    "        # print logs\n",
    "        for attr in not_accepted.keys():\n",
    "            print(\"Encountered \" + attr + \" \" + str(not_accepted[attr]) + \" times but it isn't accepted\")\n",
    "        \n",
    "        for i in range(len(self.frequencies)):\n",
    "            print(accepted_values[i]+\" is \"+str(round(100*self.frequencies[i]/sum(self.frequencies), 2))+\"% of data\")\n",
    "        \n",
    "        # making frequencies relative\n",
    "        summa = sum(self.frequencies)\n",
    "        for i in self.frequencies:\n",
    "            i = 1/i\n",
    "        \n",
    "        print(\"Dataset of \", len(self.face), \" records initialised.\")\n",
    "    def __getitem__(self, index):\n",
    "        return self.face[index], self.label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majd osszuk szét az adatot tanító-, teszt- és validációs adatbázisok között, 70:15:15 arányban. Használjunk az ellenőrzés kedvéért egy meghatározott random magot. Rendezzük a képeket kötegekbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(attribute, accepted_values, batch_size = 20):\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    dataset = MyDataset(attribute, accepted_values)\n",
    "    dataset_size = len(dataset)\n",
    "    num_val = int(np.floor(0.15 * dataset_size))\n",
    "    num_test = int(np.floor(0.15 * dataset_size))\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    val_indices = indices[:num_val]\n",
    "    test_indices = indices[num_val:num_val+num_test]\n",
    "    train_indices = indices[num_val+num_test:]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    testloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    validationloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler)\n",
    "    return trainloader, testloader, validationloader, dataset.frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanítás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osztály a validációs adatok reprezentációjára, és a validáció eredményének tárolására."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self, correct, total, balanced_acc, acc_best_num, balanced_acc_best_num, confusion_matrix,\n",
    "                 values, best_num):\n",
    "        self.correct = correct\n",
    "        self.total = total\n",
    "        self.balanced_acc = balanced_acc\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.balanced_acc_best_num = balanced_acc_best_num\n",
    "        self.values = values\n",
    "        self.best_num = best_num\n",
    "        self.acc_best_num = acc_best_num\n",
    "    def __str__(self):\n",
    "        to_print = \"\"\n",
    "        to_print += 'Accuracy: ' + str(round(100 * self.correct / self.total, 4)) + \"%\\n\"\n",
    "        to_print += \"Accuracy for balanced classes: \" + str(round(100*self.balanced_acc, 2)) + \"%\\n\"\n",
    "        to_print += \"Best \" + str(self.best_num) + \" accuracy: \" + str(round(100*self.acc_best_num, 4)) + \"%\\n\"\n",
    "        to_print += \"Best \" + str(self.best_num) + \" accuracy for balanced classes: \"\n",
    "        to_print += str(round(100*self.balanced_acc_best_num, 2)) + \"%\\n\"\n",
    "        for i in self.values:\n",
    "            to_print += \"\\t\"+i[:5]\n",
    "        to_print += \"\\n\"\n",
    "        for row in range(len(self.confusion_matrix)):\n",
    "            summa = sum(self.confusion_matrix[row])\n",
    "            to_print += self.values[row][:5]+\"\\t\"\n",
    "            for i in range(len(self.confusion_matrix[row])):\n",
    "                #confusion_matrix[row][i] = round(100*confusion_matrix[row][i]/summa, 2)\n",
    "                to_print += str(round(100*self.confusion_matrix[row][i]/summa, 2)) + \"\\t\"\n",
    "            to_print += \"\\n\"\n",
    "        return to_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Függvény a validációra. Fontos, hogy a tesztadatok és a validációs adatok elkülönüljenek a végső tesztelésnél, ám ez a függvény a tanulás közben, a tesztadatokon való tesztelésre is használható."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network, validationloader, values, best_num=1):\n",
    "    correct = total = 0\n",
    "    confusion_matrix = []\n",
    "    best_num_stats = [0]*len(values)\n",
    "    for i in range(len(values)):\n",
    "        confusion_matrix.append([0]*len(values))\n",
    "    network.train(mode=False)\n",
    "    with torch.no_grad():\n",
    "        for data in validationloader:\n",
    "            images, labels = data\n",
    "            if torch.cuda.is_available():\n",
    "                network = network.cuda()\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = network(images)\n",
    "            #print(outputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += images.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if best_num > 1:\n",
    "                for pred, lab in zip(outputs, labels):\n",
    "                    #print(torch.topk(pred, best_num, sorted=False).indices)\n",
    "                    if lab.item() in torch.topk(pred, best_num, sorted=False).indices:\n",
    "                        best_num_stats[lab.item()] += 1\n",
    "            for pred, label in zip(predicted, labels):\n",
    "                confusion_matrix[label.item()][pred.item()] += 1\n",
    "    \n",
    "    balanced_acc = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        balanced_acc += (confusion_matrix[i][i] / sum(confusion_matrix[i]))\n",
    "    balanced_acc /= len(confusion_matrix)\n",
    "    \n",
    "    balanced_acc_best_num = balanced_acc\n",
    "    if best_num > 1:\n",
    "        balanced_acc_best_num = 0\n",
    "        for i in range(len(values)):\n",
    "            balanced_acc_best_num += best_num_stats[i] / sum(confusion_matrix[i]) / len(values)\n",
    "    acc_best_num = correct/total\n",
    "    if best_num > 1:\n",
    "        acc_best_num = sum(best_num_stats)/total\n",
    "    \n",
    "    return Result(correct, total, balanced_acc, acc_best_num, balanced_acc_best_num, confusion_matrix, values,\n",
    "                  best_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Függvény egy háló betanítására.\n",
    "Példányosítsuk a neurális hálót és a segédosztályait. A hiba kiszámítására CrossEntropyLosst-t használok, mivel az egyes osztályok elemszáma nagyon változó, és ez az osztály támogatja a súlyozást, így kiegyenlítve a különböző osztályok fontosságát, elkerülve, hogy mindig a leggyakoribb osztályt tippelje a háló.\n",
    "És tanítsunk. Minden epoch után jegyezzük fel, hogy alakul a pontosság a tanulóadatokon és a tesztadatokon. Ha a tesztadatokon már romlik a pontosság, de a tanulóadaton még nő, akkor overfitting áll fent - a háló lényegében \"bemagolja\" az adatokat, ahelyett, hogy általános szabályokat találna ki. Emiatt a pontossága ismeretlen képeken csökken, így ilyenkor le kell állítani a tanítást.\n",
    "Mentsük el a modellt, hogy később tanítás nélkül is lehessen használni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nC, nFeat, nLevels, layersPerLevel, kernelSize, dropout, trainloader, testloader, lr, wd, numEpoch,\n",
    "          frequencies, stats_required = True):\n",
    "    start_time = time.time()\n",
    "    stats = []\n",
    "    losses = []\n",
    "    np.random.seed(4)\n",
    "    torch.manual_seed(4)\n",
    "\n",
    "    # initialising network\n",
    "    myNet = NeuralNetwork(1, nC, nFeat, nLevels, layersPerLevel, kernelSize, dropout)\n",
    "    optimizer = torch.optim.Adam(myNet.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.CrossEntropyLoss(torch.Tensor(frequencies))\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        myNet = myNet.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, numEpoch)\n",
    "    \n",
    "    # training\n",
    "    for i in range(numEpoch):\n",
    "        myNet.train(mode=True)\n",
    "        running_loss = 0\n",
    "        for data in trainloader:\n",
    "            inputs = data[0]\n",
    "            labels = data[1]\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = myNet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.sum().item()\n",
    "        \n",
    "        # testing the results\n",
    "        losses.append(running_loss)\n",
    "        result = None\n",
    "        if stats_required or i == numEpoch-1:\n",
    "            result = validate(myNet, testloader, frequencies)\n",
    "            stats.append(result.balanced_acc)\n",
    "            print( str(i+1) + \" / \" + str(numEpoch) + '\\tbalanced accuracy: ' +\n",
    "                  str(round(100*result.balanced_acc, 2)) + \"%\\tloss: \" + str(round(running_loss, 2)), end=\"\\r\")\n",
    "        scheduler.step()\n",
    "        \n",
    "    # printing results\n",
    "    print( str(i+1) + \" / \" + str(numEpoch) + '\\tbalanced accuracy: ' +\n",
    "            str(round(100*result.balanced_acc, 2)) + \"%\\tloss: \" + str(round(running_loss, 2)))\n",
    "    print('Finished training after ', round(time.time() - start_time), \" seconds\")\n",
    "    \n",
    "    return myNet, stats, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Függvény a betanított neurális hálón tetszőleges kép osztályozásra, hogy a gyakorlatban is használhassuk az eredményt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image, model, values, size=64):\n",
    "    faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    gray = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    for scale in range(11, 25):\n",
    "        faces = faceCascade.detectMultiScale(gray, scaleFactor=scale/10, minNeighbors=5,\n",
    "                                         minSize=(25, 25), flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "        if len(faces) == 1:\n",
    "            break\n",
    "    x, y, w, h = faces[0]\n",
    "    size = max(w, h)\n",
    "    cropped = gray[y:y + size, x:x + size]\n",
    "    face = cv2.resize(cropped, (size, size))\n",
    "    cv2.imwrite(\"test_face.png\", face)\n",
    "    Image(filename=\"test_face.png\")\n",
    "    face = cv2.normalize(face, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    batch = torch.Tensor(face).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        batch = batch.cuda()\n",
    "    \n",
    "    model.train(mode=False)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "        to_print = \"\"\n",
    "        for i in range(len(output[0])):\n",
    "            to_print += values[i] + \":\\t\" + str(output[0][i].item()) + \"\\n\"\n",
    "        print(to_print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds]",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
