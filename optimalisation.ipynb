{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparaméterek optimalizációja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A paraméteroptimalizációhoz a Bayesi optimalizációs technikát alkalmazom. Ehhez meg kell hívni a szükséges könyvtárat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from model.ipynb\n",
      "CUDA available!\n"
     ]
    }
   ],
   "source": [
    "#!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sys\n",
    "sys.path.append('c:\\\\users\\\\ifjto\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages')\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from model import ResidualBlock, NeuralNetwork, MyDataset, get_data, train, classify, validate\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizálás a nemek felismeréséhez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az optimalizálandó paraméterek értékkészletét állítsuk be. Természetesen nem megengedhetőek az óriási értékek, mivel nincs rá elegendő számítási kapacitás - áldozatot kell hoznunk. Ezen persze segíthetünk azzal, hogy több kicsi optimalizálást futtatunk, és megpróbáljuk kitalálni, hogy melyik paraméterértékek túl kicsik vagy nagyok, így szűkítve az optimalizáló értékkészletét. Néhány magyarázat a választott határokhoz:\n",
    "- nFeat: tapasztalatok azt mutatják, hogy ha 1-nek választjuk, akkor lényegesen romlik a háló pontossága\n",
    "- nLevels: 1 szintű hálónak nincs sok értelme, 6 felett pedig 1x1-es konvolúció lépne fel, ami szintén értelmetlen\n",
    "- bSize: a választott értéket 2 kitevőjére teszem, így mindig 2 hatványt próbálunk ki. Ez azért jó, mert gyorsabb lehet a tanítás\n",
    "- a kategorikus változókat kézzel választom ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'nFeat' : (3, 15),\n",
    "           'nLevels' : (2, 5),\n",
    "           'layersPerLevel' : (2, 6),\n",
    "           'kernelSize' : (3, 8),\n",
    "           'dropout' : (0, 0.6),\n",
    "           'bSize' : (4, 6),\n",
    "           'lr' : (-5.5, -2),\n",
    "           'decay' : (-9, -3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Készítsünk egy függvényt, amit az optimalizálónk tud használni. A függvény felelőssége legyen, hogy csak elfogadható értékekkel híva a train-t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_train(nFeat, nLevels, layersPerLevel, kernelSize, dropout, bSize, lr, decay):\n",
    "    nFeat = int(round(nFeat))\n",
    "    nLevels = int(round(nLevels))\n",
    "    layersPerLevel = int(round(layersPerLevel))\n",
    "    kernelSize = int(round(kernelSize)//2*2+1)\n",
    "    bSize = int(pow(2, round(bSize)))\n",
    "    numEpoch = 30\n",
    "    lr = pow(10, lr)\n",
    "    decay = pow(10, decay)\n",
    "    \n",
    "    # redirecting stdout to avoid disturbing prints\n",
    "    original_out = sys.stdout\n",
    "    sys.stdout = open(\"temp.txt\", \"a\")\n",
    "    \n",
    "    trainloader, testloader, validationloader, frequencies = get_data(\"gender\", (\"Male\", \"Female\"), bSize)\n",
    "    net, stats, losses = train(2, nFeat, nLevels, layersPerLevel, kernelSize, dropout, trainloader, testloader,\n",
    "                               lr, decay, numEpoch, frequencies)\n",
    "    \n",
    "    # resetting the stdout\n",
    "    sys.stdout = original_out\n",
    "    return stats[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimalizáljunk. A jó eredményhez kezdjük néhány random lépéssel a próbálkozást."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   bSize   |   decay   |  dropout  | kernel... | layers... |    lr     |   nFeat   |  nLevels  | numEpoch  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  1        |  0.4776   |  4.834    | -4.678    |  6.862e-0 |  4.512    |  2.587    | -7.446    |  4.304    |  3.037    |  22.76    |\n",
      "|  2        |  0.5038   |  5.078    | -6.485    |  0.4111   |  4.022    |  5.512    | -7.836    |  7.693    |  3.252    |  24.7     |\n",
      "|  3        |  0.8822   |  4.281    | -7.811    |  0.4804   |  7.841    |  3.254    | -3.846    |  9.135    |  4.684    |  19.02    |\n",
      "|  4        |  0.7766   |  4.078    | -7.981    |  0.5269   |  3.492    |  3.684    | -2.253    |  6.732    |  4.076    |  21.79    |\n",
      "|  5        |  0.8046   |  5.373    | -3.992    |  0.01097  |  6.751    |  5.955    | -3.511    |  4.963    |  4.368    |  19.24    |\n",
      "|  6        |  0.662    |  6.0      | -3.0      |  0.0      |  3.0      |  2.0      | -2.0      |  10.0     |  5.0      |  18.0     |\n",
      "|  7        |  0.5583   |  6.0      | -9.0      |  0.0      |  8.0      |  6.0      | -2.0      |  3.0      |  5.0      |  30.0     |\n",
      "|  8        |  0.6804   |  4.0      | -9.0      |  0.0      |  8.0      |  6.0      | -2.0      |  10.0     |  2.0      |  18.0     |\n",
      "|  9        |  0.5057   |  6.0      | -9.0      |  0.6      |  3.0      |  6.0      | -8.0      |  3.0      |  5.0      |  18.0     |\n",
      "|  10       |  0.5994   |  4.0      | -3.0      |  0.6      |  8.0      |  2.0      | -2.0      |  10.0     |  5.0      |  30.0     |\n",
      "|  11       |  0.5771   |  4.0      | -9.0      |  0.6      |  8.0      |  2.0      | -2.0      |  3.0      |  5.0      |  18.0     |\n",
      "|  12       |  0.5      |  4.0      | -3.0      |  0.6      |  8.0      |  6.0      | -8.0      |  10.0     |  5.0      |  18.0     |\n",
      "|  13       |  0.6234   |  6.0      | -9.0      |  0.0      |  8.0      |  2.0      | -2.0      |  10.0     |  5.0      |  24.35    |\n",
      "|  14       |  0.7682   |  4.0      | -3.0      |  0.6      |  3.0      |  6.0      | -2.0      |  3.0      |  5.0      |  30.0     |\n",
      "|  15       |  0.6166   |  4.0      | -3.0      |  0.6      |  8.0      |  6.0      | -2.0      |  3.0      |  5.0      |  24.75    |\n",
      "|  16       |  0.6504   |  4.0      | -9.0      |  0.0      |  3.0      |  6.0      | -2.0      |  10.0     |  5.0      |  30.0     |\n",
      "|  17       |  0.6594   |  4.0      | -3.0      |  0.6      |  3.0      |  6.0      | -2.0      |  3.0      |  5.0      |  18.0     |\n",
      "|  18       |  0.7086   |  6.0      | -9.0      |  0.0      |  3.0      |  6.0      | -2.0      |  10.0     |  5.0      |  18.0     |\n",
      "|  19       |  0.833    |  6.0      | -3.0      |  0.6      |  3.0      |  6.0      | -2.0      |  10.0     |  2.0      |  30.0     |\n",
      "|  20       |  0.8348   |  6.0      | -3.0      |  0.6      |  3.0      |  2.0      | -2.0      |  3.0      |  2.0      |  30.0     |\n",
      "|  21       |  0.6659   |  6.0      | -3.0      |  0.6      |  8.0      |  2.0      | -2.0      |  7.519    |  2.0      |  18.0     |\n",
      "|  22       |  0.4976   |  6.0      | -9.0      |  0.0      |  8.0      |  2.0      | -8.0      |  10.0     |  5.0      |  18.0     |\n",
      "|  23       |  0.5057   |  6.0      | -4.708    |  0.6      |  8.0      |  6.0      | -2.0      |  10.0     |  5.0      |  19.43    |\n",
      "|  24       |  0.5      |  5.587    | -8.203    |  0.4345   |  7.361    |  2.919    | -7.689    |  8.02     |  4.897    |  19.09    |\n",
      "|  25       |  0.8396   |  4.0      | -9.0      |  0.0      |  3.0      |  6.0      | -2.0      |  3.0      |  2.0      |  30.0     |\n",
      "=====================================================================================================================================\n",
      "{'target': 0.8822400454927792, 'params': {'bSize': 4.280773877190468, 'decay': -7.811391065490727, 'dropout': 0.480446741205322, 'kernelSize': 7.841307878596988, 'layersPerLevel': 3.2536967126369714, 'lr': -3.8460643059841155, 'nFeat': 9.134724066072268, 'nLevels': 4.683819990511542, 'numEpoch': 19.020530536437334}}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer = BayesianOptimization(\n",
    "    f=gender_train,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=20,\n",
    ")\n",
    "\n",
    "print(optimizer.max)\n",
    "print('Finished after ', round(time.time() - start_time), \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizálás a rasszok felismeréséhez\n",
    "\n",
    "Az előzőkhöz hasonlóan járunk el, de az értékkészleten változtatunk, ha kell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'nFeat' : (19, 21),\n",
    "           'nLevels' : (2, 3),\n",
    "           'layersPerLevel' : (2, 3),\n",
    "           'kernelSize' : (3, 9),\n",
    "           'dropout' : (0.05, 0.5),\n",
    "           'bSize' : (4, 5),\n",
    "           'lr' : (-4.3, -3.5),\n",
    "           'decay' : (-9, -3)}\n",
    "\n",
    "def race_train(nFeat, nLevels, layersPerLevel, kernelSize, dropout, bSize, lr, decay):\n",
    "    nFeat = int(round(nFeat))\n",
    "    nLevels = int(round(nLevels))\n",
    "    layersPerLevel = int(round(layersPerLevel))\n",
    "    kernelSize = int(round(kernelSize)//2*2+1)\n",
    "    bSize = int(pow(2, round(bSize)))\n",
    "    numEpoch = 30\n",
    "    lr = pow(10, lr)\n",
    "    decay = pow(10, decay)\n",
    "    \n",
    "    # redirecting stdout to avoid disturbing prints\n",
    "    original_out = sys.stdout\n",
    "    sys.stdout = open(\"temp.txt\", \"a\")\n",
    "    \n",
    "    accepted_values = (\"Black\", \"White\", \"Asian\")\n",
    "    trainloader, testloader, validationloader, frequencies = get_data(\"race\", accepted_values, bSize)\n",
    "    net, stats, losses = train(len(accepted_values), nFeat, nLevels, layersPerLevel, kernelSize, dropout, trainloader,\n",
    "                               testloader, lr, decay, numEpoch, frequencies)\n",
    "    \n",
    "    # resetting the stdout\n",
    "    sys.stdout = original_out\n",
    "    return stats[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "És optimalizáljunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   bSize   |   decay   |  dropout  | kernel... | layers... |    lr     |   nFeat   |  nLevels  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|  1        |  0.3604   |  4.417    | -4.678    |  5.719e-0 |  4.814    |  2.294    | -4.751    |  16.12    |  2.346    |\n",
      "|  2        |  0.3492   |  4.397    | -5.767    |  0.2096   |  7.111    |  2.409    | -2.629    |  15.16    |  2.67     |\n",
      "|  3        |  0.3503   |  4.417    | -5.648    |  0.07019  |  4.189    |  3.601    | -2.386    |  16.88    |  2.692    |\n",
      "|  4        |  0.4913   |  4.876    | -3.632    |  0.04252  |  3.234    |  2.34     | -2.629    |  15.59    |  2.421    |\n",
      "|  5        |  0.3333   |  4.958    | -5.801    |  0.3459   |  4.893    |  3.373    | -2.747    |  15.11    |  2.75     |\n",
      "|  6        |  0.5294   |  4.989    | -4.511    |  0.1402   |  7.736    |  2.206    | -3.791    |  20.45    |  2.294    |\n",
      "|  7        |  0.4854   |  4.288    | -8.22     |  0.009683 |  7.073    |  2.423    | -4.283    |  17.95    |  2.053    |\n",
      "|  8        |  0.522    |  4.574    | -8.12     |  0.2947   |  7.199    |  2.205    | -3.882    |  19.17    |  2.414    |\n",
      "|  9        |  0.4573   |  4.05     | -5.785    |  0.3319   |  6.089    |  3.889    | -3.416    |  20.42    |  2.137    |\n",
      "|  10       |  0.4234   |  4.139    | -4.156    |  0.1988   |  3.992    |  3.855    | -4.061    |  19.5     |  2.726    |\n",
      "|  11       |  0.3874   |  5.0      | -9.0      |  0.5      |  3.0      |  2.0      | -2.3      |  21.0     |  2.0      |\n",
      "|  12       |  0.4133   |  5.0      | -9.0      |  0.0      |  9.0      |  4.0      | -5.0      |  21.0     |  3.0      |\n",
      "|  13       |  0.3407   |  5.0      | -3.0      |  0.5      |  3.0      |  2.0      | -2.3      |  21.0     |  2.0      |\n",
      "|  14       |  0.3465   |  4.0      | -9.0      |  0.0      |  9.0      |  2.0      | -2.3      |  21.0     |  2.0      |\n",
      "|  15       |  0.3333   |  5.0      | -3.0      |  0.5      |  9.0      |  4.0      | -5.0      |  17.34    |  2.0      |\n",
      "|  16       |  0.4398   |  4.0      | -3.0      |  0.5      |  9.0      |  2.0      | -5.0      |  21.0     |  3.0      |\n",
      "|  17       |  0.3333   |  5.0      | -9.0      |  0.0      |  5.017    |  2.0      | -5.0      |  21.0     |  2.0      |\n",
      "|  18       |  0.3328   |  5.0      | -9.0      |  0.0      |  3.0      |  2.0      | -2.3      |  15.0     |  3.0      |\n",
      "|  19       |  0.3953   |  5.0      | -9.0      |  0.5      |  9.0      |  2.0      | -5.0      |  15.0     |  3.0      |\n",
      "|  20       |  0.3333   |  5.0      | -3.0      |  0.5      |  9.0      |  4.0      | -2.3      |  21.0     |  3.0      |\n",
      "|  21       |  0.3333   |  5.0      | -6.703    |  0.5      |  9.0      |  2.0      | -5.0      |  19.31    |  2.0      |\n",
      "|  22       |  0.3312   |  5.0      | -9.0      |  0.0      |  9.0      |  4.0      | -2.3      |  16.95    |  3.0      |\n",
      "|  23       |  0.3476   |  5.0      | -3.0      |  0.0      |  6.205    |  2.0      | -2.3      |  18.21    |  3.0      |\n",
      "|  24       |  0.3333   |  4.0      | -3.0      |  0.5      |  3.0      |  4.0      | -5.0      |  15.0     |  2.0      |\n",
      "|  25       |  0.3672   |  4.0      | -6.979    |  0.0      |  5.467    |  2.0      | -2.3      |  21.0     |  3.0      |\n",
      "|  26       |  0.3784   |  5.0      | -3.0      |  0.0      |  6.519    |  4.0      | -5.0      |  21.0     |  2.0      |\n",
      "|  27       |  0.3333   |  4.0      | -9.0      |  0.5      |  3.0      |  4.0      | -5.0      |  18.71    |  3.0      |\n",
      "|  28       |  0.3683   |  4.0      | -3.0      |  0.5      |  3.0      |  2.0      | -2.3      |  15.0     |  2.0      |\n",
      "|  29       |  0.3333   |  5.0      | -3.0      |  0.5      |  3.0      |  2.0      | -5.0      |  18.08    |  3.0      |\n",
      "|  30       |  0.338    |  5.0      | -9.0      |  0.0      |  5.759    |  4.0      | -2.3      |  21.0     |  3.0      |\n",
      "|  31       |  0.4054   |  4.0      | -5.292    |  0.0      |  8.975    |  4.0      | -4.548    |  19.76    |  3.0      |\n",
      "|  32       |  0.4377   |  5.0      | -6.267    |  0.0      |  3.0      |  2.0      | -3.265    |  18.27    |  2.0      |\n",
      "|  33       |  0.4223   |  5.0      | -3.0      |  0.0      |  9.0      |  2.0      | -5.0      |  15.0     |  3.0      |\n",
      "|  34       |  0.4478   |  5.0      | -3.0      |  0.0      |  3.0      |  4.0      | -2.3      |  15.0     |  2.0      |\n",
      "|  35       |  0.3651   |  5.0      | -3.0      |  0.0      |  9.0      |  2.0      | -2.396    |  21.0     |  2.0      |\n",
      "|  36       |  0.3333   |  5.0      | -9.0      |  0.5      |  6.425    |  2.0      | -2.3      |  17.47    |  2.0      |\n",
      "|  37       |  0.4664   |  4.0      | -9.0      |  0.0      |  8.379    |  2.0      | -4.6      |  18.57    |  3.0      |\n",
      "|  38       |  0.4139   |  4.0      | -9.0      |  0.0      |  9.0      |  4.0      | -5.0      |  15.0     |  3.0      |\n",
      "|  39       |  0.4488   |  5.0      | -5.709    |  0.5      |  6.605    |  2.741    | -4.403    |  21.0     |  3.0      |\n",
      "|  40       |  0.4133   |  5.0      | -3.0      |  0.0      |  3.0      |  2.0      | -3.791    |  15.0     |  3.0      |\n",
      "|  41       |  0.5385   |  4.0      | -9.0      |  0.5      |  7.458    |  4.0      | -4.244    |  19.78    |  2.0      |\n",
      "|  42       |  0.3503   |  5.0      | -3.0      |  0.0      |  9.0      |  4.0      | -2.3      |  15.0     |  2.0      |\n",
      "|  43       |  0.3333   |  4.0      | -9.0      |  0.0      |  3.0      |  4.0      | -5.0      |  15.0     |  2.0      |\n",
      "|  44       |  0.5285   |  4.0      | -3.957    |  0.5      |  6.906    |  2.0      | -3.92     |  21.0     |  2.0      |\n",
      "|  45       |  0.3953   |  5.0      | -9.0      |  0.5      |  7.17     |  4.0      | -5.0      |  18.08    |  3.0      |\n",
      "|  46       |  0.4584   |  5.0      | -3.0      |  0.0      |  3.0      |  4.0      | -3.227    |  17.94    |  2.0      |\n",
      "|  47       |  0.4054   |  4.0      | -3.0      |  0.0      |  9.0      |  4.0      | -5.0      |  15.0     |  3.0      |\n",
      "|  48       |  0.5024   |  4.0      | -5.883    |  0.0      |  6.974    |  2.0      | -4.043    |  20.15    |  2.0      |\n",
      "|  49       |  0.3423   |  4.0      | -9.0      |  0.5      |  9.0      |  4.0      | -3.277    |  21.0     |  3.0      |\n",
      "|  50       |  0.5104   |  5.0      | -9.0      |  0.0      |  7.926    |  3.348    | -3.801    |  19.58    |  2.0      |\n",
      "|  51       |  0.3423   |  4.0      | -9.0      |  0.0      |  9.0      |  2.0      | -2.923    |  15.0     |  2.0      |\n",
      "|  52       |  0.5199   |  4.0      | -9.0      |  0.0      |  6.286    |  2.867    | -3.723    |  19.84    |  2.0      |\n",
      "|  53       |  0.3768   |  5.0      | -5.319    |  0.0      |  3.0      |  2.0      | -2.3      |  15.0     |  2.0      |\n",
      "|  54       |  0.3858   |  5.0      | -3.797    |  0.0      |  3.0      |  2.0      | -2.3      |  17.34    |  3.0      |\n",
      "|  55       |  0.4234   |  5.0      | -5.804    |  0.5      |  3.0      |  4.0      | -4.062    |  21.0     |  2.0      |\n",
      "|  56       |  0.3333   |  4.0      | -3.0      |  0.0      |  3.0      |  2.0      | -5.0      |  21.0     |  2.0      |\n",
      "|  57       |  0.4573   |  4.0      | -3.0      |  0.5      |  9.0      |  2.0      | -3.599    |  15.0     |  2.0      |\n",
      "|  58       |  0.4578   |  5.0      | -3.0      |  0.0      |  5.556    |  2.293    | -3.256    |  15.0     |  2.0      |\n",
      "|  59       |  0.3333   |  4.0      | -3.0      |  0.5      |  9.0      |  2.0      | -2.3      |  15.0     |  3.0      |\n",
      "|  60       |  0.5294   |  4.0      | -3.0      |  0.0      |  7.259    |  2.0      | -3.731    |  21.0     |  3.0      |\n",
      "|  61       |  0.4371   |  5.0      | -9.0      |  0.0      |  3.0      |  4.0      | -2.685    |  18.45    |  2.0      |\n",
      "|  62       |  0.5135   |  4.0      | -3.0      |  0.0      |  7.496    |  2.3      | -4.103    |  19.24    |  2.0      |\n",
      "|  63       |  0.4658   |  4.0      | -9.0      |  0.5      |  3.0      |  4.0      | -2.995    |  21.0     |  2.0      |\n",
      "|  64       |  0.4859   |  4.0      | -4.358    |  0.5      |  7.908    |  2.0      | -3.49     |  19.83    |  3.0      |\n",
      "|  65       |  0.4674   |  5.0      | -3.0      |  0.0      |  6.316    |  2.0      | -3.937    |  20.99    |  2.0      |\n",
      "|  66       |  0.4589   |  5.0      | -3.723    |  0.5      |  3.0      |  3.06     | -3.224    |  16.28    |  2.0      |\n",
      "|  67       |  0.4584   |  4.0      | -9.0      |  0.0      |  7.308    |  4.0      | -5.0      |  21.0     |  2.0      |\n",
      "|  68       |  0.4652   |  4.0      | -3.0      |  0.5      |  7.777    |  3.336    | -3.579    |  21.0     |  2.0      |\n",
      "|  69       |  0.4929   |  4.022    | -8.063    |  0.07969  |  7.34     |  3.299    | -3.495    |  19.02    |  2.016    |\n",
      "|  70       |  0.4398   |  5.0      | -7.02     |  0.5      |  7.182    |  2.0      | -3.104    |  21.0     |  2.0      |\n",
      "|  71       |  0.4669   |  4.0      | -9.0      |  0.5      |  7.454    |  2.0      | -4.417    |  19.59    |  2.0      |\n",
      "|  72       |  0.4213   |  4.0      | -5.004    |  0.0      |  3.0      |  4.0      | -2.3      |  21.0     |  2.0      |\n",
      "|  73       |  0.3959   |  4.0      | -5.154    |  0.0      |  9.0      |  2.0      | -4.994    |  15.0     |  2.0      |\n",
      "|  74       |  0.4494   |  4.0      | -9.0      |  0.5      |  3.0      |  2.0      | -3.613    |  18.39    |  2.0      |\n",
      "|  75       |  0.5114   |  4.0      | -3.0      |  0.0      |  3.0      |  2.921    | -3.415    |  16.73    |  2.0      |\n",
      "=========================================================================================================================\n",
      "{'target': 0.5385169549855457, 'params': {'bSize': 4.0, 'decay': -9.0, 'dropout': 0.5, 'kernelSize': 7.457915539176346, 'layersPerLevel': 4.0, 'lr': -4.244144260718087, 'nFeat': 19.777632561533398, 'nLevels': 2.0}}\n",
      "Finished after  59579  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer = BayesianOptimization(\n",
    "    f=race_train,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=10,\n",
    "    n_iter=65,\n",
    ")\n",
    "\n",
    "print(optimizer.max)\n",
    "print('Finished after ', round(time.time() - start_time), \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizálás a munkakörök felismeréséhez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'nFeat' : (5, 21),\n",
    "           'nLevels' : (2, 5),\n",
    "           'layersPerLevel' : (2, 5),\n",
    "           'kernelSize' : (3, 9),\n",
    "           'dropout' : (0, 0.5),\n",
    "           'bSize' : (4, 5),\n",
    "           'lr' : (-8, -2),\n",
    "           'decay' : (-9, -3)}\n",
    "\n",
    "def occupation_train(nFeat, nLevels, layersPerLevel, kernelSize, dropout, bSize, lr, decay):\n",
    "    nFeat = int(round(nFeat))\n",
    "    nLevels = int(round(nLevels))\n",
    "    layersPerLevel = int(round(layersPerLevel))\n",
    "    kernelSize = int(round(kernelSize)//2*2+1)\n",
    "    bSize = int(pow(2, round(bSize)))\n",
    "    numEpoch = 30\n",
    "    lr = pow(10, lr)\n",
    "    decay = pow(10, decay)\n",
    "    \n",
    "    # redirecting stdout to avoid disturbing prints\n",
    "    original_out = sys.stdout\n",
    "    sys.stdout = open(\"temp.txt\", \"a\")\n",
    "    \n",
    "    accepted_values = (\"Star\", \"Sciences\", \"Sports\", \"Arts\", \"Business/politics\", \"Military\", \"Religion\", \n",
    "                  \"Crime\", \"Music\", \"Law\")\n",
    "    trainloader, testloader, validationloader, frequencies = get_data(\"occupation_category\", accepted_values, bSize)\n",
    "    net, stats, losses = train(len(accepted_values), nFeat, nLevels, layersPerLevel, kernelSize, dropout, trainloader,\n",
    "                               testloader, lr, decay, numEpoch, frequencies)\n",
    "    \n",
    "    # resetting the stdout\n",
    "    sys.stdout = original_out\n",
    "    return stats[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   bSize   |   decay   |  dropout  | kernel... | layers... |    lr     |   nFeat   |  nLevels  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|  1        |  0.1104   |  4.417    | -4.678    |  5.719e-0 |  4.814    |  2.44     | -7.446    |  7.98     |  3.037    |\n",
      "|  2        |  0.1401   |  4.397    | -5.767    |  0.2096   |  7.111    |  2.613    | -2.731    |  5.438    |  4.011    |\n",
      "|  3        |  0.1534   |  4.417    | -5.648    |  0.07019  |  4.189    |  4.402    | -2.19     |  10.01    |  4.077    |\n",
      "|  4        |  0.1644   |  4.876    | -3.632    |  0.04252  |  3.234    |  2.509    | -2.731    |  6.574    |  3.263    |\n",
      "|  5        |  0.1476   |  4.958    | -5.801    |  0.3459   |  4.893    |  4.06     | -2.992    |  5.293    |  4.25     |\n",
      "|  6        |  0.1673   |  4.682    | -9.0      |  0.0362   |  3.0      |  2.0      | -2.0      |  5.96     |  2.798    |\n",
      "|  7        |  0.1437   |  4.961    | -3.484    |  0.5      |  3.0      |  2.98     | -2.0      |  21.0     |  5.0      |\n",
      "|  8        |  0.1439   |  4.35     | -3.286    |  0.3241   |  4.583    |  4.84     | -2.012    |  7.015    |  2.071    |\n",
      "|  9        |  0.1348   |  4.223    | -6.397    |  0.1305   |  3.082    |  2.574    | -2.349    |  6.373    |  4.982    |\n",
      "|  10       |  0.1419   |  5.0      | -9.0      |  0.0      |  9.0      |  2.0      | -2.0      |  21.0     |  2.0      |\n",
      "|  11       |  0.1341   |  4.858    | -3.264    |  0.1102   |  8.816    |  2.172    | -2.051    |  14.45    |  3.579    |\n",
      "|  12       |  0.1561   |  4.966    | -7.312    |  0.1773   |  3.165    |  2.725    | -2.125    |  15.19    |  2.071    |\n",
      "|  13       |  0.09542  |  4.941    | -8.628    |  0.09664  |  3.063    |  4.734    | -7.694    |  20.79    |  4.015    |\n",
      "|  14       |  0.1443   |  4.725    | -8.801    |  0.165    |  8.937    |  4.56     | -2.608    |  10.05    |  2.406    |\n",
      "|  15       |  0.1436   |  4.961    | -8.498    |  0.02476  |  4.288    |  4.631    | -2.04     |  17.02    |  4.993    |\n",
      "|  16       |  0.1395   |  4.88     | -3.006    |  0.2836   |  3.7      |  4.585    | -4.4      |  20.64    |  2.311    |\n",
      "|  17       |  0.1488   |  4.978    | -7.642    |  0.05788  |  6.317    |  2.789    | -2.249    |  5.689    |  2.401    |\n",
      "|  18       |  0.1      |  4.707    | -3.509    |  0.2137   |  8.889    |  4.677    | -6.974    |  20.43    |  4.55     |\n",
      "|  19       |  0.1069   |  4.866    | -3.42     |  0.1029   |  8.995    |  4.296    | -7.823    |  6.421    |  4.086    |\n",
      "|  20       |  0.136    |  4.705    | -8.715    |  0.2213   |  8.749    |  2.202    | -2.41     |  10.03    |  4.941    |\n",
      "|  21       |  0.1221   |  5.0      | -3.0      |  0.5      |  8.586    |  5.0      | -2.043    |  21.0     |  3.882    |\n",
      "|  22       |  0.142    |  4.048    | -8.69     |  0.009795 |  3.714    |  4.695    | -2.175    |  19.71    |  2.016    |\n",
      "|  23       |  0.1469   |  4.99     | -3.14     |  0.4918   |  5.305    |  2.334    | -2.202    |  10.15    |  3.619    |\n",
      "|  24       |  0.1376   |  4.908    | -3.64     |  0.2169   |  8.142    |  4.737    | -2.001    |  8.171    |  4.873    |\n",
      "|  25       |  0.09294  |  4.81     | -8.882    |  0.4944   |  3.121    |  4.863    | -6.399    |  8.385    |  2.132    |\n",
      "|  26       |  0.1677   |  4.851    | -3.351    |  0.1543   |  3.281    |  4.845    | -2.212    |  11.82    |  2.09     |\n",
      "|  27       |  0.1327   |  4.201    | -8.627    |  0.03918  |  8.844    |  4.902    | -2.721    |  5.27     |  4.453    |\n",
      "|  28       |  0.1586   |  4.175    | -3.18     |  0.0984   |  3.164    |  4.634    | -2.391    |  17.22    |  4.12     |\n",
      "|  29       |  0.1432   |  4.996    | -8.846    |  0.3979   |  3.357    |  4.542    | -2.105    |  13.06    |  2.543    |\n",
      "|  30       |  0.1373   |  4.904    | -7.451    |  0.3071   |  8.928    |  4.652    | -2.42     |  16.55    |  4.482    |\n",
      "|  31       |  0.147    |  4.75     | -3.069    |  0.007767 |  4.868    |  3.477    | -2.004    |  18.12    |  2.107    |\n",
      "|  32       |  0.1533   |  4.861    | -3.233    |  0.1335   |  4.117    |  4.725    | -4.696    |  12.11    |  4.671    |\n",
      "|  33       |  0.1779   |  4.778    | -3.416    |  0.05585  |  3.354    |  2.429    | -3.079    |  13.53    |  2.713    |\n",
      "|  34       |  0.1      |  4.068    | -3.697    |  0.006187 |  3.045    |  2.025    | -6.315    |  18.86    |  3.433    |\n",
      "|  35       |  0.1547   |  4.128    | -3.906    |  0.09665  |  3.841    |  2.012    | -2.139    |  12.53    |  2.213    |\n",
      "|  36       |  0.1603   |  4.818    | -3.177    |  0.06302  |  3.158    |  3.035    | -2.101    |  14.66    |  4.639    |\n",
      "|  37       |  0.1661   |  4.954    | -8.12     |  0.08302  |  8.834    |  2.465    | -5.17     |  13.38    |  3.751    |\n",
      "|  38       |  0.1      |  4.69     | -4.103    |  0.05274  |  8.938    |  3.315    | -6.14     |  12.98    |  2.114    |\n",
      "|  39       |  0.1688   |  4.979    | -8.785    |  0.2868   |  8.836    |  3.915    | -4.514    |  19.61    |  4.925    |\n",
      "|  40       |  0.1153   |  4.99     | -8.95     |  0.3582   |  8.622    |  2.414    | -5.88     |  5.344    |  3.969    |\n",
      "|  41       |  0.2035   |  4.021    | -8.67     |  0.1039   |  8.648    |  3.115    | -4.634    |  20.87    |  4.765    |\n",
      "|  42       |  0.1      |  4.585    | -8.942    |  0.08813  |  8.634    |  2.316    | -7.662    |  20.02    |  4.238    |\n",
      "|  43       |  0.164    |  4.085    | -8.888    |  0.02573  |  8.997    |  3.64     | -4.406    |  20.83    |  4.732    |\n",
      "|  44       |  0.1607   |  4.0      | -9.0      |  0.5      |  3.078    |  2.0      | -3.844    |  21.0     |  5.0      |\n",
      "|  45       |  0.1557   |  4.241    | -5.86     |  0.4982   |  7.065    |  2.718    | -3.769    |  20.62    |  4.976    |\n",
      "|  46       |  0.1544   |  4.209    | -8.775    |  0.4568   |  8.357    |  2.089    | -3.856    |  17.68    |  4.912    |\n",
      "|  47       |  0.1055   |  4.0      | -3.0      |  0.0      |  3.0      |  5.0      | -8.0      |  5.0      |  2.0      |\n",
      "|  48       |  0.1382   |  5.0      | -9.0      |  0.0      |  3.0      |  2.0      | -5.671    |  11.6     |  5.0      |\n",
      "|  49       |  0.1983   |  5.0      | -9.0      |  0.0      |  6.823    |  2.0      | -4.217    |  21.0     |  5.0      |\n",
      "|  50       |  0.1      |  5.0      | -8.528    |  4.141e-1 |  8.733    |  5.0      | -7.268    |  10.55    |  5.0      |\n",
      "|  51       |  0.1466   |  5.0      | -3.0      |  0.0      |  9.0      |  2.0      | -2.0      |  5.0      |  2.0      |\n",
      "|  52       |  0.1045   |  4.068    | -7.971    |  0.3199   |  3.031    |  4.528    | -7.553    |  5.109    |  4.755    |\n",
      "|  53       |  0.17     |  4.44     | -6.436    |  0.4563   |  7.795    |  2.247    | -4.365    |  20.75    |  2.068    |\n",
      "|  54       |  0.1167   |  4.0      | -4.205    |  2.428e-0 |  3.0      |  2.0      | -4.743    |  5.0      |  2.0      |\n",
      "|  55       |  0.1741   |  4.832    | -6.581    |  0.1058   |  8.784    |  2.406    | -5.192    |  20.54    |  4.32     |\n",
      "|  56       |  0.1577   |  5.0      | -9.0      |  0.0      |  3.0      |  5.0      | -2.0      |  5.0      |  2.405    |\n",
      "|  57       |  0.1424   |  5.0      | -3.0      |  0.5      |  3.0      |  2.0      | -8.0      |  11.48    |  5.0      |\n",
      "|  58       |  0.1      |  5.0      | -5.671    |  0.5      |  3.0      |  5.0      | -6.881    |  15.77    |  5.0      |\n",
      "|  59       |  0.1493   |  5.0      | -3.0      |  0.5      |  3.0      |  5.0      | -2.0      |  21.0     |  2.0      |\n",
      "|  60       |  0.1665   |  4.151    | -8.842    |  0.3312   |  5.848    |  3.301    | -4.726    |  20.46    |  3.633    |\n",
      "|  61       |  0.08944  |  5.0      | -3.0      |  0.0      |  3.0      |  2.0      | -8.0      |  5.0      |  5.0      |\n",
      "|  62       |  0.1      |  4.0      | -7.261    |  0.0      |  9.0      |  5.0      | -6.03     |  5.0      |  2.0      |\n",
      "|  63       |  0.173    |  4.955    | -6.267    |  0.4548   |  8.82     |  2.427    | -4.527    |  10.46    |  4.96     |\n",
      "|  64       |  0.1484   |  4.876    | -7.42     |  0.04427  |  3.09     |  2.2      | -2.314    |  8.739    |  2.455    |\n",
      "|  65       |  0.1107   |  4.876    | -3.027    |  0.07273  |  3.229    |  4.972    | -5.048    |  5.29     |  4.455    |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  66       |  0.1507   |  5.0      | -6.579    |  0.5      |  7.618    |  4.689    | -3.309    |  12.93    |  2.449    |\n",
      "|  67       |  0.1772   |  4.063    | -3.238    |  0.3533   |  8.912    |  2.378    | -4.354    |  20.89    |  2.48     |\n",
      "|  68       |  0.1364   |  4.044    | -8.274    |  0.3804   |  7.607    |  2.39     | -2.595    |  20.83    |  4.887    |\n",
      "|  69       |  0.1317   |  4.201    | -3.489    |  0.172    |  3.054    |  4.808    | -5.395    |  20.64    |  4.75     |\n",
      "|  70       |  0.1472   |  4.0      | -9.0      |  0.5      |  3.0      |  2.0      | -2.0      |  21.0     |  2.0      |\n",
      "=========================================================================================================================\n",
      "{'target': 0.2034742849971351, 'params': {'bSize': 4.021404677708202, 'decay': -8.669600781396264, 'dropout': 0.10388419665685261, 'kernelSize': 8.648116853863876, 'layersPerLevel': 3.1154707291179657, 'lr': -4.634232320937716, 'nFeat': 20.87164490006776, 'nLevels': 4.765290329759951}}\n",
      "Finished after  81521  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer = BayesianOptimization(\n",
    "    f=occupation_train,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=65,\n",
    ")\n",
    "\n",
    "print(optimizer.max)\n",
    "print('Finished after ', round(time.time() - start_time), \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds]",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
